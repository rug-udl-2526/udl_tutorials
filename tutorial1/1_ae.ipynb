{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a60dd3-6423-492f-9e43-0e165b38bf08",
   "metadata": {},
   "source": [
    "# 1.1 Autoencoders\n",
    "\n",
    "In this tutorial we will familiarize ourselves with Autoencoders (AEs) and how they work in practice. AEs are the common encoder-decoder architecture, where the input is projected into a lower dimensional (latent) space, which is then fully reconstructed. The latent space is learned with the aim to capture similarities between the inputs and can be used for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a78d6-566a-402f-b012-7e51e490213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.distributions import Normal\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61027d35-d61a-4bdb-802b-f3fe4ef98b6d",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will be experimenting with the popular MNIST dataset provided by `torchvision`. It contains 60 000, 28x28 grayscale images of handwritten digits. We can download it directly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a817d43-8c8a-43fc-ba01-873587914abf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist = MNIST('data', transform=ToTensor(), download=True)\n",
    "dataloader = DataLoader(mnist, batch_size=100, shuffle=True)\n",
    "len(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af82ae-802e-442a-bc35-f714a8a5b415",
   "metadata": {},
   "source": [
    "Here to each sample we apply the `ToTensor()` transformation, which simply scales the input between 0 and 1 and converts the images to pytorch tensors. Since we don't have enough memory to pass the entire dataset into our model, we will use batches of size 100, utilizing the `DataLoader` from pytorch. The following shows an example image from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26e081-0f54-499f-a1c7-0808b87194a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(idx: int) -> None:\n",
    "    image = mnist[idx][0].squeeze()\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_image(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de822ffe",
   "metadata": {},
   "source": [
    "## AE\n",
    "\n",
    "We can define our autoencoder. The encoder is a compression function $\\textbf{z}=encoder(\\textbf{x})$, mapping the data $\\textbf{x}  \\in \\mathbb{R}^d \\to \\textbf{z} \\in \\mathbb{R}^p$. The reconstructed representation is reconstructed to $\\hat{\\textbf{x}}$ by the decoder \n",
    "\n",
    "The encoder and decoder are regular feed forward neural networks. Our latent space will consist of $p$ distributions. We will be using $p=2$ in order to visualize the latent space later. Notice that this is a reduction of 99.75% (a 784-dimensional feature vector is reduced to only 2 numbers) so we cannot expect super impressive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAE(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: implement the encoder\n",
    "        self.encoder = \n",
    "\n",
    "        # TODO: implement the decoder\n",
    "        self.decoder = \n",
    "        \n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        z = self.encoder(x)\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.decoder(z)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # TODO: implement the forward pass\n",
    "\n",
    "        return reconstruction\n",
    "    \n",
    "ae = CustomAE()\n",
    "optimizer = torch.optim.Adam(params=ae.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2124f073",
   "metadata": {},
   "source": [
    "### Training\n",
    "Now we can get to training our model! Feel free to reduce the number of epochs or the size of the model. After training, the model weights will be saved at `custom_ae.pt` (~ 3MB) to make sure that you do not need to retrain the model every time you restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efacefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'custom_ae.pt'\n",
    "if os.path.exists(model_path):\n",
    "    ae.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "else:\n",
    "    epochs = 15\n",
    "    for epoch in range(epochs):\n",
    "        ae.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (x, _) in tqdm(enumerate(dataloader)):\n",
    "            x = x.flatten(1) # transform (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "            optimizer.zero_grad()\n",
    "            reconstruction= ae(x)\n",
    "            loss = F.binary_cross_entropy(reconstruction, x, reduction='sum')\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {train_loss / len(dataloader.dataset):.4f}\")\n",
    "    torch.save(ae.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8afa02",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We can see that our model is definitely learning! Let's visualize a reconstruction of a training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a83d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_image(idx: int = 0) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    image = mnist[idx][0].flatten(1)\n",
    "    with torch.inference_mode():\n",
    "        reconstruction= ae(image)\n",
    "    return image.squeeze().reshape(28, 28), reconstruction.squeeze().reshape(28, 28)\n",
    "\n",
    "def show_reconstruction(original_image: torch.Tensor, reconstruction: torch.Tensor) -> None:\n",
    "    fig, ax = plt.subplots(ncols=2)\n",
    "    ax[0].imshow(original_image, cmap='gray')\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('Original')\n",
    "    ax[1].imshow(reconstruction, cmap='gray')\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('Reconstruction')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "idx = 10\n",
    "original, reconstruction = reconstruct_image(idx)\n",
    "show_reconstruction(original, reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98848c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_space():\n",
    "    # How many images to show per row and column\n",
    "    rows, cols = 7, 7\n",
    "\n",
    "    # Samples of the latent space\n",
    "    xs = torch.linspace(-1, 1, cols)\n",
    "    ys = torch.linspace(-1, 1, rows)\n",
    "    samples = torch.stack([torch.stack((x, y)) for x in xs for y in ys])\n",
    "    \n",
    "    # Calculate the reconstructions\n",
    "    with torch.inference_mode():\n",
    "        images = ae.decode(samples).reshape(rows, cols, 28, 28)\n",
    "\n",
    "    # Concatenate all reconstructions into a grid\n",
    "    grid = torch.cat([torch.cat([images[i, j] for i in range(rows)], dim=1) for j in range(cols)], dim=0)\n",
    "    \n",
    "    # Plot them\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    xticks, yticks = range(14, (cols) * 28 + 14, 28), range(14, (rows) * 28 + 14, 28)\n",
    "    xlabels, ylabels = xs.round(decimals=3).numpy(), ys.round(decimals=3).numpy()\n",
    "    plt.xticks(ticks=xticks, labels=xlabels)\n",
    "    plt.yticks(ticks=yticks, labels=ylabels)\n",
    "    plt.xlabel('z1')\n",
    "    plt.ylabel('z2')\n",
    "    plt.imshow(grid,cmap='gray')\n",
    "    plt.title('Latent Space Visualization')\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
