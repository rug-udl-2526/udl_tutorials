{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a60dd3-6423-492f-9e43-0e165b38bf08",
   "metadata": {},
   "source": [
    "# 1.1 Variational Autoencoders\n",
    "\n",
    "In this tutorial we will familiarize ourselves with Variational Autoencoders (VAEs) and how they work in practice. VAEs are an extension to the common encoder-decoder architecture, where the input is projected into a lower dimensional (latent) space, which is then fully reconstructed. The latent space is learned with the aim to capture similarities between the inputs and can be used for dimensionality reduction. VAEs provide an additional flexibility, where the input is encoded into probability distributions instead of scalars. We can also sample from these distributions to generate new datapoints, which hopefully resemble the original ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a78d6-566a-402f-b012-7e51e490213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.distributions import Normal\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61027d35-d61a-4bdb-802b-f3fe4ef98b6d",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will be experimenting with the popular MNIST dataset provided by `torchvision`. It contains 60 000, 28x28 grayscale images of handwritten digits. We can download it directly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a817d43-8c8a-43fc-ba01-873587914abf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist = MNIST('data', transform=ToTensor(), download=True)\n",
    "dataloader = DataLoader(mnist, batch_size=100, shuffle=True)\n",
    "len(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af82ae-802e-442a-bc35-f714a8a5b415",
   "metadata": {},
   "source": [
    "Here to each sample we apply the `ToTensor()` transformation, which simply scales the input between 0 and 1 and converts the images to pytorch tensors. Since we don't have enough memory to pass the entire dataset into our model, we will use batches of size 100, utilizing the `DataLoader` from pytorch. The following shows an example image from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26e081-0f54-499f-a1c7-0808b87194a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(idx: int) -> None:\n",
    "    image = mnist[idx][0].squeeze()\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_image(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62294e-ed3a-4855-bed6-8098a8d9eea1",
   "metadata": {},
   "source": [
    "## VAE\n",
    "\n",
    "Now we can define our variational autoencoder. The encoder and decoder are regular feed forward neural networks. Feel free to experiment with the parameters! Our latent space will consist of $n$ independent univariate normal distributions (equivalently, an $n$-variate normal distribution with diagonal covariance), each characterized by a mean $\\mu$ and a standard deviation $\\sigma$. We split the network at the end of our encoder in 2: one layer for predicting the means, and one layer for predicting the standard deviations. Then, we can simply sample from the resulting distributions and reconstruct the samples using the decoder. We will be using $n=2$ in order to visualize the latent space later. Notice that this is a reduction of 99.75% (a 784-dimensional feature vector is reduced to only 2 numbers) so we cannot expect super impressive results. At the end of the tutorial, try increasing the size of the latent space and see how the reconstructions change!\n",
    "\n",
    "Of course, there is a catch! Sampling from a distribution is a stochastic non-differentiable operation. Therefore, as soon as we sample, we lose track of the gradient and we cannot train our encoder. To circumvent this, we will be using the *reparameterization trick*, which entails generating pseudo-samples while still preserving the gradient. Given $\\mathcal{N}(\\mu, \\sigma)$, we can generate samples $z$ as follows:\n",
    "\n",
    "$$\\epsilon\\sim\\mathcal{N}(0, 1)$$\n",
    "$$z=\\mu + \\sigma\\epsilon$$\n",
    "\n",
    "Here we only scale the standard deviation and add to it the mean, which are bot differentiable operations! You can convince yourself that the operation above is exactly the same as $z\\sim\\mathcal{N}(\\mu,\\sigma)$. An intuition that might help you is that the operation above resembles the inverse of the $z$-score, where given a random variable $x$ with mean $\\mu$ and standadrd deviation $\\sigma$, we can set the mean to 0 and the standard deviation to 1, while preserving all other properties of data, with the following transformation:\n",
    "\n",
    "$$z(x)=\\frac{x-\\mu}{\\sigma}$$\n",
    "\n",
    "Above we do the opposite, we sample from $\\mathcal{N}(0,1)$, then we set the mean to $\\mu$ by adding it, and set the standard deviation to $\\sigma$ by scaling by it! We can also simply visualize the 2 resulting distributions to verify that they are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbf601-7531-44fd-a8bb-54ed75215ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 5., 2.\n",
    "mu_tensor, sigma_tensor = torch.tensor(mu, requires_grad=True), torch.tensor(sigma, requires_grad=True)\n",
    "real_dist = Normal(mu_tensor, sigma_tensor)\n",
    "eps = Normal(torch.tensor(0.), torch.tensor(1.))\n",
    "\n",
    "n_samples = 100_000\n",
    "real_samples = real_dist.sample((n_samples,))\n",
    "pseudo_samples = mu_tensor + sigma_tensor * eps.sample((n_samples,))\n",
    "\n",
    "print(real_samples.grad_fn, pseudo_samples.grad_fn)\n",
    "\n",
    "sns.kdeplot(real_samples, label='Real')\n",
    "sns.kdeplot(pseudo_samples.detach(), label='Reparameterization')\n",
    "plt.legend()\n",
    "plt.title('Comparison of real sampling with reparameterization')\n",
    "plt.xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db49f0-0aee-40d9-8775-bd8aa2e7992d",
   "metadata": {},
   "source": [
    "Another catch is that the standard deviation of a distribution must always be a non-negative number. To ensure that is the case, we use $\\text{softplus}$ as an activation function to the layer responsible for the standard deviations. And that's it! Now we can define our VAE simply as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5755db49-7a72-45c0-b39c-47f63a21a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVAE(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mu = nn.Linear(200, 2)\n",
    "        self.sigma = nn.Sequential(nn.Linear(200, 2), nn.Softplus())\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.encoder(x)\n",
    "        mean = self.mu(x)\n",
    "        std = self.sigma(x)\n",
    "        return mean, std\n",
    "\n",
    "    def reparameterize(self, mean: torch.Tensor, std: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: implement the reparameterization trick\n",
    "\n",
    "        return pseudo_sample\n",
    "\n",
    "    def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # TODO: implement the forward pass\n",
    "\n",
    "        return reconstruction, mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce5d4fe-8ddb-45e8-8e04-6ab4fc934b89",
   "metadata": {},
   "source": [
    "Before we begin training, we need to define a custom loss function. As you know from the lectures, if we use only the reconstruction loss (in our case simply the binary cross entropy), the model can *cheat* by learning a very narrow distribution. However, we want diverse samples from which we can also generate new diverse datapoints. The loss function is extended using the Kullback-Leibler divergence regularization to ensure that the learned distributions are pulled to a *prior*, which we define as another normal distribution $\\mathcal{N}(0,\\sigma_0)$, where $\\sigma_0$ is a hyperparameter, commonly set to 1.\n",
    "\n",
    "The KL-divergence is fully differentiable and is defined as follows for normal distributions $p$ and $q$ (for derivations, see [here](https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians)):\n",
    "\n",
    "$$KL(p,q)=\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{\\sigma_1^2+(\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$$\n",
    "\n",
    "The more similar $p$ and $q$ are, the smaller $KL(p,q)$ is. Therefore, we can simply add $KL(p,q)$ to the reconstruction loss as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15696b-d8e3-48c7-9817-30060a061c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DKL(mean: torch.Tensor, std: torch.Tensor, mean_prior: float = 0., std_prior: float = 1.) -> torch.Tensor:\n",
    "    # TODO: implement the DKL calculation\n",
    "    \n",
    "    return dkl \n",
    "\n",
    "def custom_loss(\n",
    "    x: torch.Tensor, \n",
    "    reconstruction: torch.Tensor, \n",
    "    mean: torch.Tensor, \n",
    "    std: torch.Tensor, \n",
    "    mean_prior: float = 0.,\n",
    "    std_prior: float = 1.\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    bce = F.binary_cross_entropy(reconstruction, x, reduction='sum')\n",
    "    dkl = DKL(mean, std, mean_prior, std_prior)\n",
    "    return bce + dkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11388888-12f4-48d9-983e-fe956f2a7c6c",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now we can get to training our model! Since we are using a big dataset, the training may take a while. Feel free to reduce the number of epochs or the size of the model. The process is the same as usual. Also, after training, the model weights will be saved at `custom_vae.pt` (~ 3MB) to make sure that you do not need to retrain the model every time you restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d117c-22a9-4ae5-9a23-f71d3b959b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = CustomVAE()\n",
    "optimizer = torch.optim.Adam(params=vae.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b65200-5e36-4dcb-917f-4faed87bd72b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = 'custom_vae.pt'\n",
    "if os.path.exists(model_path):\n",
    "    vae.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "else:\n",
    "    epochs = 15\n",
    "    for epoch in range(epochs):\n",
    "        vae.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (x, _) in tqdm(enumerate(dataloader)):\n",
    "            x = x.flatten(1) # transform (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "            optimizer.zero_grad()\n",
    "            reconstruction, mean, std = vae(x)\n",
    "            loss = custom_loss(x, reconstruction, mean, std)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {train_loss / len(dataloader.dataset):.4f}\")\n",
    "    torch.save(vae.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422f610-13ec-4e14-b16c-155dc215f92b",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We can see that our model is definitely learning! Let's visualize a reconstruction of a training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c67675-38e7-4b35-9e8f-71de623d95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_image(idx: int = 0) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    image = mnist[idx][0].flatten(1)\n",
    "    with torch.inference_mode():\n",
    "        reconstruction, mean, std = vae(image)\n",
    "    return image.squeeze().reshape(28, 28), reconstruction.squeeze().reshape(28, 28), mean.squeeze(), std.squeeze()\n",
    "\n",
    "def show_reconstruction(original_image: torch.Tensor, reconstruction: torch.Tensor) -> None:\n",
    "    fig, ax = plt.subplots(ncols=2)\n",
    "    ax[0].imshow(original_image, cmap='gray')\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('Original')\n",
    "    ax[1].imshow(reconstruction, cmap='gray')\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('Reconstruction')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "idx = 10\n",
    "original, reconstruction, mean, std = reconstruct_image(idx)\n",
    "show_reconstruction(original, reconstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea732f88-1006-4b81-ad58-779dd92a4ad0",
   "metadata": {},
   "source": [
    "It seems that our model is doing quite a decent job! Let us now visualize the latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a2a8e-a275-4bd0-9644-298b29bd6032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_space():\n",
    "\n",
    "    # How many images to show per row and column\n",
    "    rows, cols = 7, 7\n",
    "\n",
    "    # Samples of the latent space\n",
    "    xs = torch.linspace(-1, 1, cols)\n",
    "    ys = torch.linspace(-1, 1, rows)\n",
    "    samples = torch.stack([torch.stack((x, y)) for x in xs for y in ys])\n",
    "    \n",
    "    # Calculate the reconstructions\n",
    "    with torch.inference_mode():\n",
    "        images = vae.decode(samples).reshape(rows, cols, 28, 28)\n",
    "\n",
    "    # Concatenate all reconstructions into a grid\n",
    "    grid = torch.cat([torch.cat([images[i, j] for i in range(rows)], dim=1) for j in range(cols)], dim=0)\n",
    "    \n",
    "    # Plot them\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    xticks, yticks = range(14, (cols) * 28 + 14, 28), range(14, (rows) * 28 + 14, 28)\n",
    "    xlabels, ylabels = xs.round(decimals=3).numpy(), ys.round(decimals=3).numpy()\n",
    "    plt.xticks(ticks=xticks, labels=xlabels)\n",
    "    plt.yticks(ticks=yticks, labels=ylabels)\n",
    "    plt.xlabel('z1')\n",
    "    plt.ylabel('z2')\n",
    "    plt.imshow(grid,cmap='gray')\n",
    "    plt.title('Latent Space Visualization')\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1166fe-b248-41e9-a587-7ff5b0ac16c2",
   "metadata": {},
   "source": [
    "This looks really cool! Compare the latent space exploration between the VAE and the AE. Notice that the transitions between generated samples are smoother with the VAE than with the standard AE.\n",
    "\n",
    "Experiment with the parameters, i.e. the range of the samples in the latent space (e.g. not between -1 and 1 but try -5 to 5) and the resolution (not 15 by 15 but try 20 by 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b2d97-1ad5-43bc-9b82-02ba9349de5c",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "For the rest of the tutorial you can implement a discrete VAE using the Gumbel-softmax trick. In our current implementation the latent space of the VAE is continuous. However, we are often interested in generating images of discrete classes (such as here, digits 0-9). Instead of encoding the input into $n$ normal distributions with means and standard deviations, we can encode it instead into $n$ categorical distributions with 10 categories. To implement this, the last layer of the encoder could be a linear layer that projects the input onto a $10n$-dimensional space. The result can be then simply reshaped reshaped into a $n\\times 10$ matrix, where the $i$-th row are the logits of the $i$-th categorical distribution.\n",
    "\n",
    "Great! Now how do we sample from a categorical distribution while preserving the gradient? The key is to use the Gumbel-softmax trick! The Gumbel distribution is defined as follows:\n",
    "\n",
    "$$\\text{Gumbel}=-\\log\\left(-\\log\\left(U\\left(0,1\\right)\\right)\\right)$$\n",
    "\n",
    "Where $U(0,1)$ is the uniform distribution between 0 and 1. We can add Gumbel noise to each category in each distribution and \"sample\" using the $\\text{softmax}$ with temperature $\\tau$ as follows:\n",
    "\n",
    "$$z_i=\\text{softmax}\\left(\\frac{\\log(\\alpha_i) + \\text{Gumbel}}{\\tau}\\right)$$\n",
    "\n",
    "Here $\\alpha_i$ is the 10 logits of the $i$-th categorical distribution and the $\\log$ function is applied element-wise. We can then concatenate the resulting $n$ 10-dimensional vectors $z_i$ and directly pass them to the decoder. Here we are simply taking the log element-wise, adding a constant, dividing by a constant, and taking the softmax, which are all differentiable operations! Also, we are using *soft* sampling, where instead of picking a single category, we pick a probability distribution over the categories. This is important since picking a single category is non-differentiable.\n",
    "\n",
    "In this case we want our prior to be uniform distribution. To achieve that we can replace our regularization term with the formula for the KL divergence between 2 categorical distributions $p$ and $q$, which is as follows:\n",
    "\n",
    "$$KL(p,q)=\\sum_jp_j\\log\\left(\\frac{p_j}{q_j}\\right)$$\n",
    "\n",
    "Where $p_j$ and $q_j$ are the respective probabilities for the $j$-th category. A uniform categorical distribution with $C$ categories is defined as $p_i=\\frac{1}{C}, i=1\\ldots C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068b1d11-f23c-4e03-a980-0e969ebc07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
